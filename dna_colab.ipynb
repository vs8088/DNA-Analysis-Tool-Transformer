{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e2d81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title ðŸ§¬ DNA Analysis Tool (Nucleotide Transformer Colab Edition)\n",
    "# @markdown Run this cell to start the app. **Ensure you have selected T4 GPU runtime.**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from google.colab import files\n",
    "\n",
    "# --- 1. Install Dependencies ---\n",
    "if not os.environ.get(\"SKIP_PIP_INSTALL\"):\n",
    "    print(\"Installing dependencies (one-time; please wait)...\")\n",
    "    !pip install -q transformers torch pandas huggingface_hub[hf_xet] hf_transfer sentencepiece\n",
    "else:\n",
    "    print(\"Skipping dependency install (SKIP_PIP_INSTALL=1).\")\n",
    "print(\"Dependencies step finished. UI will appear below.\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# --- 2. Core Analysis Class ---\n",
    "class DNAAnalyzerEngine:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        # Configuration for sequence processing\n",
    "        self.max_length = 1000\n",
    "        self.chunk_overlap = 50\n",
    "        \n",
    "    def load_model(self, use_fp16=True, status_callback=None):\n",
    "        \"\"\"Loads model and tokenizer with optional FP16 optimization.\"\"\"\n",
    "        try:\n",
    "            if status_callback: status_callback(\"Downloading Tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "            \n",
    "            if status_callback: status_callback(\"Downloading Model Weights (This takes time)...\")\n",
    "            \n",
    "            # Use FP16 if requested and on CUDA to save VRAM\n",
    "            torch_dtype = torch.float16 if use_fp16 and self.device == \"cuda\" else torch.float32\n",
    "            \n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(\n",
    "                self.model_name, \n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch_dtype\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            return True, f\"Loaded on {self.device.upper()} ({'FP16' if use_fp16 else 'FP32'})\"\n",
    "        except Exception as e:\n",
    "            return False, str(e)\n",
    "\n",
    "    def calculate_perplexity(self, sequence):\n",
    "        \"\"\"Calculates perplexity using sliding window for long sequences.\"\"\"\n",
    "        inputs = self.tokenizer(sequence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        input_ids = inputs[\"input_ids\"][0]\n",
    "        \n",
    "        # Chunking Strategy\n",
    "        chunks = []\n",
    "        if len(input_ids) <= self.max_length:\n",
    "            chunks.append(input_ids)\n",
    "        else:\n",
    "            # Sliding window\n",
    "            stride = self.max_length - self.chunk_overlap\n",
    "            for i in range(0, len(input_ids), stride):\n",
    "                chunk = input_ids[i : i + self.max_length]\n",
    "                if len(chunk) > 10: chunks.append(chunk)\n",
    "\n",
    "        if not chunks: return None\n",
    "\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for chunk in chunks:\n",
    "                chunk = chunk.unsqueeze(0).to(self.device)\n",
    "                # Masked LM loss where labels = inputs\n",
    "                outputs = self.model(chunk, labels=chunk)\n",
    "                loss = outputs.loss\n",
    "                if not torch.isnan(loss):\n",
    "                    total_loss += loss.item()\n",
    "                    count += 1\n",
    "        \n",
    "        if count == 0: return None\n",
    "        return torch.exp(torch.tensor(total_loss / count)).item()\n",
    "\n",
    "    def repair_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Repairs DNA sequence by predicting masked 'N' tokens.\n",
    "        Fix: Handles 6-mer expansion by truncating prediction to 1 base.\n",
    "        \"\"\"\n",
    "        # Basic validation\n",
    "        if not sequence or 'N' not in sequence.upper():\n",
    "            return sequence, \"No 'N' found\"\n",
    "\n",
    "        # 1. Identify all 'N' positions for precise repair.\n",
    "        # For simplicity/efficiency, we maintain the masked replacement logic\n",
    "        # but will handle length control during the decoding phase.\n",
    "        \n",
    "        mask_str = self.tokenizer.mask_token\n",
    "        # Note: Simple replacement makes the tokenizer see \"...ACGT[MASK]ACGT...\"\n",
    "        # The NT tokenizer splits surrounding ACGT into bases or fragments, \n",
    "        # while [MASK] is treated as a 6-mer to be predicted.\n",
    "        masked_seq_str = sequence.upper().replace('N', mask_str)\n",
    "        \n",
    "        inputs = self.tokenizer(masked_seq_str, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        \n",
    "        # Length check\n",
    "        if input_ids.shape[1] > self.max_length:\n",
    "            return sequence, \"Skipped (Too Long)\"\n",
    "\n",
    "        # Model Inference\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids).logits\n",
    "\n",
    "        # Find all mask positions\n",
    "        mask_indices = (input_ids == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "        if len(mask_indices[0]) == 0:\n",
    "             return sequence, \"Tokenizer Error\"\n",
    "\n",
    "        # Get predicted token IDs\n",
    "        predicted_ids = logits[mask_indices].argmax(dim=-1)\n",
    "        \n",
    "        # --- Critical Fix Start ---\n",
    "        # We cannot directly replace input_ids with predicted_ids and decode, \n",
    "        # as that would insert the entire 6-mer (causing sequence length expansion).\n",
    "        \n",
    "        # We need to manually construct the repaired string.\n",
    "        # Due to complex tokenizer behavior, the most robust method is:\n",
    "        # 1. Convert input_ids back to a token list.\n",
    "        # 2. For each [MASK] position, decode its predicted ID and take only the first character.\n",
    "        \n",
    "        # Convert original input_ids to list (assuming batch size is 1 here)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        \n",
    "        # Create a pointer to iterate through predicted_ids\n",
    "        pred_idx = 0\n",
    "        \n",
    "        restored_tokens = []\n",
    "        for token in tokens:\n",
    "            if token == mask_str:\n",
    "                # This is a masked position; retrieve the corresponding predicted ID\n",
    "                pred_id = predicted_ids[pred_idx]\n",
    "                pred_idx += 1\n",
    "                \n",
    "                # Decode this token the full predicted 6-mer (e.g., might decode to \"ACGTGC\")\n",
    "                pred_str = self.tokenizer.decode(pred_id)\n",
    "                \n",
    "                # Take only the first base (assuming 'N' represents a single point deletion)\n",
    "                # Note: If 'N' represents a larger segment, keeping the full 6-mer might be better.\n",
    "                # But in this \"Deep Repair\" context, we enforce 1:1 replacement.\n",
    "                fix_base = pred_str[0] if pred_str else \"N\" \n",
    "                restored_tokens.append(fix_base)\n",
    "            else:\n",
    "                restored_tokens.append(token)\n",
    "        \n",
    "        # Reassemble the string\n",
    "        # Use convert_tokens_to_string to handle any subword prefixes (like ##) correctly\n",
    "        repaired_seq = self.tokenizer.convert_tokens_to_string(restored_tokens)\n",
    "        \n",
    "        # Clean up any spaces or residual special tokens\n",
    "        repaired_seq = repaired_seq.replace(\" \", \"\").replace(mask_str, \"\")\n",
    "        \n",
    "        # --- Critical Fix End ---\n",
    "        \n",
    "        return repaired_seq, \"Repaired\"\n",
    "\n",
    "\n",
    "# --- 3. Colab UI Logic ---\n",
    "\n",
    "# Model Selection Dropdown\n",
    "# IMPORTANT: NT-500M Human Ref is set as default for Human Detection\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"NT-500M Human Ref (Best for Human Detection)\", \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"),\n",
    "        (\"NT-2.5B 1000G (Best Accuracy for Human Variants)\", \"InstaDeepAI/nucleotide-transformer-2.5b-1000g\"),\n",
    "        (\"NT-2.5B Multi-species (Best for Repair/General)\", \"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\"),\n",
    "    ],\n",
    "    value=\"InstaDeepAI/nucleotide-transformer-500m-human-ref\",\n",
    "    description='Model:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Initialize engine with default model\n",
    "engine = DNAAnalyzerEngine(model_dropdown.value)\n",
    "uploaded_filename = None\n",
    "\n",
    "# UI Widgets\n",
    "header = widgets.HTML(\"<h2>ðŸ§¬ DNA Analysis Tool (Colab Edition)</h2>\")\n",
    "status_label = widgets.Label(value=\"System Status: Idle (Waiting for file upload)\")\n",
    "upload_btn = widgets.FileUpload(accept='.csv', multiple=False)\n",
    "col_name_input = widgets.Text(value='sequence_text', description='Col Name:')\n",
    "mode_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Classify (Is Human DNA?)', 'classify'), \n",
    "        ('Deep Repair (Fill N)', 'repair')\n",
    "    ], \n",
    "    description='Mode:'\n",
    ")\n",
    "load_btn = widgets.Button(description=\"Initialize Model\", button_style='primary', icon='download')\n",
    "run_btn = widgets.Button(description=\"Start Analysis\", button_style='success', icon='play', disabled=True)\n",
    "output_area = widgets.Output()\n",
    "progress_bar = widgets.IntProgress(value=0, min=0, max=100, description='Progress:', bar_style='info', orientation='horizontal', layout=widgets.Layout(width='100%'))\n",
    "\n",
    "def on_upload_change(change):\n",
    "    global uploaded_filename\n",
    "    if not upload_btn.value: return\n",
    "    \n",
    "    try:\n",
    "        # Handle new ipywidgets upload format\n",
    "        uploaded_filename = list(upload_btn.value.keys())[0]\n",
    "        content = upload_btn.value[uploaded_filename]['content']\n",
    "    except:\n",
    "        # Handle older format (sometimes tuple)\n",
    "        uploaded_file = upload_btn.value[0]\n",
    "        uploaded_filename = uploaded_file.name\n",
    "        content = uploaded_file.content\n",
    "        \n",
    "    with open(uploaded_filename, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    status_label.value = f\"File Uploaded: {uploaded_filename}\"\n",
    "\n",
    "upload_btn.observe(on_upload_change, names='value')\n",
    "\n",
    "def on_load_click(b):\n",
    "    load_btn.disabled = True\n",
    "    status_label.value = \"Initializing Model... (Please wait)\"\n",
    "    \n",
    "    # Update engine with currently selected model\n",
    "    engine.model_name = model_dropdown.value\n",
    "    \n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(f\"Initializing model: {engine.model_name}\")\n",
    "        if \"human-ref\" in engine.model_name:\n",
    "            print(\"Context: Human Reference (Suitable for detecting Human vs Contamination)\")\n",
    "        else:\n",
    "            print(\"Context: Multi-Species (Suitable for general DNA repair or embeddings)\")\n",
    "    \n",
    "    def _load():\n",
    "        success, msg = engine.load_model(status_callback=lambda s: setattr(status_label, 'value', s))\n",
    "        if success:\n",
    "            status_label.value = f\"Ready: {msg}\"\n",
    "            run_btn.disabled = False\n",
    "            with output_area:\n",
    "                print(f\"Model ready: {engine.model_name} | {msg}\")\n",
    "        else:\n",
    "            status_label.value = f\"Error: {msg}\"\n",
    "            load_btn.disabled = False\n",
    "            with output_area:\n",
    "                print(f\"Error loading model: {msg}\")\n",
    "            \n",
    "    threading.Thread(target=_load).start()\n",
    "\n",
    "load_btn.on_click(on_load_click)\n",
    "\n",
    "def on_run_click(b):\n",
    "    if not uploaded_filename:\n",
    "        status_label.value = \"Error: No CSV file uploaded!\"\n",
    "        return\n",
    "    \n",
    "    run_btn.disabled = True\n",
    "    col = col_name_input.value\n",
    "    mode = mode_dropdown.value\n",
    "    \n",
    "    # Determine context for logic interpretation\n",
    "    is_human_ref = \"human-ref\" in engine.model_name.lower() or \"500m\" in engine.model_name.lower()\n",
    "    \n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(f\"Reading {uploaded_filename}...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(uploaded_filename)\n",
    "            if col not in df.columns:\n",
    "                print(f\"Error: Column '{col}' not found.\")\n",
    "                run_btn.disabled = False\n",
    "                return\n",
    "            \n",
    "            results, details = [], []\n",
    "            total = len(df)\n",
    "            progress_bar.max = total\n",
    "            progress_bar.value = 0\n",
    "            \n",
    "            print(f\"Processing {total} rows. Mode: {mode}...\")\n",
    "            \n",
    "            # Threshold for human reference perplexity\n",
    "            human_ppl_threshold = 45.0\n",
    "            \n",
    "            for i, row in df.iterrows():\n",
    "                seq = str(row[col]).strip()\n",
    "                if not seq or seq.lower() == 'nan':\n",
    "                    results.append(\"\"); details.append(\"Empty\")\n",
    "                    continue\n",
    "                    \n",
    "                if mode == 'classify':\n",
    "                    ppl = engine.calculate_perplexity(seq)\n",
    "                    if ppl:\n",
    "                        score = round(ppl, 2)\n",
    "                        \n",
    "                        # --- INTERPRETATION LOGIC ---\n",
    "                        if is_human_ref:\n",
    "                            # For Human Ref model: Low PPL = Human, High PPL = Alien/Contamination\n",
    "                            if ppl < human_ppl_threshold:\n",
    "                                res_str = \"Likely Human DNA\"\n",
    "                            else:\n",
    "                                res_str = \"Likely Contamination/Non-Human\"\n",
    "                        else:\n",
    "                            # For Multi-species model: Low PPL = Valid DNA (could be bacteria)\n",
    "                            if ppl < human_ppl_threshold:\n",
    "                                res_str = \"Valid DNA (Species Unknown)\"\n",
    "                            else:\n",
    "                                res_str = \"High Perplexity / Noise\"\n",
    "                        \n",
    "                        results.append(res_str)\n",
    "                        details.append(score)\n",
    "                    else:\n",
    "                        results.append(\"Error\"); details.append(-1)\n",
    "                else:\n",
    "                    # Repair Mode\n",
    "                    fixed, status = engine.repair_sequence(seq)\n",
    "                    results.append(fixed); details.append(status)\n",
    "                \n",
    "                progress_bar.value = i + 1\n",
    "                if i % 10 == 0: print(f\"Row {i}/{total} processed\", end='\\r')\n",
    "            \n",
    "            # Save with timestamp to avoid collisions\n",
    "            ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            out_name = f\"processed_{mode}_{ts}_{uploaded_filename}\"\n",
    "            \n",
    "            if mode == 'classify':\n",
    "                df['classification_result'] = results\n",
    "                df['perplexity_score'] = details\n",
    "                df['model_used'] = engine.model_name\n",
    "            else:\n",
    "                df['repaired_sequence'] = results\n",
    "                df['repair_status'] = details\n",
    "            \n",
    "            df.to_csv(out_name, index=False)\n",
    "            print(f\"\\nDone! Saved to {out_name}\")\n",
    "            \n",
    "            # Trigger Download\n",
    "            files.download(out_name)\n",
    "            status_label.value = \"Analysis Complete! File downloaded.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            run_btn.disabled = False\n",
    "\n",
    "run_btn.on_click(on_run_click)\n",
    "\n",
    "# Layout Construction\n",
    "ui = widgets.VBox([\n",
    "    header,\n",
    "    widgets.HBox([upload_btn, status_label]),\n",
    "    model_dropdown,\n",
    "    widgets.HBox([col_name_input, mode_dropdown]),\n",
    "    widgets.HBox([load_btn, run_btn]),\n",
    "    progress_bar,\n",
    "    output_area\n",
    "])\n",
    "\n",
    "display(ui)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
